# -*- coding: utf-8 -*-
"""Salinan dari Prediksi_LSTM_Adaro(max).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cMO7z9YqPMkjabB1lNEt8n5rG76HRoWd

# Melakukan Prediksi dengan menggunakan model LSTM

Model LSTM (Long Short Term Memory) merupakan Deep Neural Network yang mampu melakukan prediksi pergerakan saham ataupun jenis prediksi lainnya. Untuk menggunakan model ini, ada beberapa hal yang harus diperlukan, yaitu :
1. Membuat data generator <br/>Sebelum membuat data generator, data harus dibersihkan dan dilakukan Pre-processing
2. Menentukan hyperparameter yang tepat untuk meminimalisir nilai Error <br/>Untuk menentukan nilai hyperparameter yang tepat, dapat dilihat di <a href="s.id/hyper_lstm_saham"> **sini**</a>
3. Melakukan visualisasi dan evaluasi
4. Menganalisa secara teknikal yang bisa dilihat di <a href="s.id/teknikal_lstm_saham"> **sini**</a>

# Membersihkan Data dan melakukan Pre-Processing

## Library yang digunakan <br/>
Tahap awal adalah menentukan dan mendownload library yang akan digunakan. Di bawah ini adalah library - library yang akan digunakan dalam penelitian ini
"""

pip install tqdm --upgrade

# Numpy untuk formatting number atau array
import numpy as np

# Pandas untuk membuat DataFrame dan membaca dokumen csv
import pandas as pd 

# Tensorflow dan Keras sebagai Framework dan Backend
import keras
import tensorflow as tf
from keras.models import Sequential, load_model
from keras.layers import Dense, Dropout
from keras.layers import LSTM
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger
from keras import optimizers

# Sklearn sebagai komponen machine learning, untuk normalisasi, membagi dataset dan menghitung nilai error
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Logging untuk mencatat tiap aktivitas yang dilakukan sistem
import logging

# Pickle untuk mengambil model jika sudah dibuat sebelumnya
import pickle

# OS untuk operation system, sys untuk system, time untuk waktu, tqdm notebook untuk notebook jika dibuat selain google colab
import os
import sys
import time
from tqdm._tqdm_notebook import tqdm_notebook

"""## Mengatur zona waktu dan logging
Zona waktu digunakan untuk mengetahui seberapa lama sistem akan melatih data, dan logging digunakan untuk mencatat segala aktivitas yang dilakukan
"""

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
logging.getLogger("tensorflow").setLevel(logging.ERROR)
os.environ['TZ'] = 'Asia/Kolkata'  
time.tzset()

print("Penelitian ini menggunakan Framework Keras versi " + keras.__version__ + " dan Backend Tensorflow versi " + tf.__version__)

"""## Membaca dataset dan membagi dataset
Dataset yang digunakan berupa Historical Data Price PT Adaro Energy Tbk sejak 2008 sampai 3 januari 2020
"""

from google.colab import drive
drive.mount('/content/drive')

params = {
    "batch_size": 20, 
    "epochs": 200,
    "lr": 0.0001,
    "time_steps": 50
}
iter_changes = "dropout_layers_0.4_0.4"

INPUT_PATH = "/content/drive/My Drive/TA_LSTM"+"/inputs"
OUTPUT_PATH = "/content/drive/My Drive/TA_LSTM/outputs/lstm_best_11-1-20_2PM/"+"iter_changes"
TIME_STEPS = params["time_steps"]
BATCH_SIZE = params["batch_size"]
stime = time.time()

if not os.path.exists(OUTPUT_PATH):
  os.makedirs(OUTPUT_PATH)
  print("Direktori output dibuat di ", OUTPUT_PATH)
else:
  print("Direktori sudah dibuat sebelumnya")

stime = time.time()
print(os.listdir(INPUT_PATH))

# Membaca Dataset
df = pd.read_csv(os.path.join(INPUT_PATH, "ADRO.JK(max).csv"), engine='python')
df = df.dropna()
df = df.reset_index(drop=True)
print(df.shape)
print(df.columns)
display(df.head(5))

# Menampilkan Feature dan Tipe Data yang digunakan
print("\n Tipe data pada Fitur yang digunakan :")
print(df.dtypes)

# Mendefinisikan variable baru untuk menampung Feature
train_cols = ["Open","High","Low","Close","Volume"]

# Membagi dataset menjadi 80% untuk Data Latih dan 20% untuk Data Uji
df_train, df_test = train_test_split(df, train_size=0.8, test_size=0.2, shuffle=False)
print("\nBanyaknya Data Latih : ", len(df_train))
print("Banyaknya Data Uji : ", len(df_test))

# Split skip 2 hari untuk prediksi tanggal 4, dimulai dari tanggal 2 januari, 27 desember dll
df2 = df.iloc[::2]
df2_train, df2_test = train_test_split(df2, train_size=0.8, test_size=0.2, shuffle=False)
print("\nBanyaknya Data Latih : ", len(df2_train))
print("Banyaknya Data Uji : ", len(df2_test))
df2.tail()

# Split skip 2 hari untuk prediksi tanggal 5, dimulai dari tanggal 3 januari, 30 desember dll
df3 = df.iloc[1::2]
df3_train, df3_test = train_test_split(df3, train_size=0.8, test_size=0.2, shuffle=False)
print("\nBanyaknya Data Latih : ", len(df3_train))
print("Banyaknya Data Uji : ", len(df3_test))
df3.tail()

"""## Visualisasi terhadap dataset
Visualisasi ini bertujuan untuk mengetahui OHLC (Open, High, Low, Close) dengan menampilkan nya berbentuk grafik candle stick
"""

import plotly.graph_objects as go
import pandas as pd
from datetime import datetime

fig = go.Figure(data=[go.Candlestick(x=df['Date'],
                open=df['Open'],
                high=df['High'],
                low=df['Low'],
                close=df['Close'])])
fig.show()

"""## Melakukan Normalisasi data
Normalisasi data dilakukan untuk memudahkan melakukan prediksi. Normalisasi yang digunakan berupa MinMaxScaller.
"""

# Melakukan Normalisasi untuk Data Latih dan Data Uji
x = df_train.loc[:,train_cols].values
min_max_scaler = MinMaxScaler()
x_train = min_max_scaler.fit_transform(x)
x_test = min_max_scaler.transform(df_test.loc[:,train_cols])

# Melakukan Normalisasi untuk Data Latih dan Data Uji
x2 = df2_train.loc[:,train_cols].values
min_max_scaler = MinMaxScaler()
x2_train = min_max_scaler.fit_transform(x2)
x2_test = min_max_scaler.transform(df2_test.loc[:,train_cols])

# Melakukan Normalisasi untuk Data Latih dan Data Uji
x3 = df3_train.loc[:,train_cols].values
min_max_scaler = MinMaxScaler()
x3_train = min_max_scaler.fit_transform(x3)
x3_test = min_max_scaler.transform(df3_test.loc[:,train_cols])

"""## Menghapus data yang tidak diperlukan"""

print("Menghapus dataframe yang tidak terpakai . . .")
apus  = (sys.getsizeof(df)+sys.getsizeof(df_train)+sys.getsizeof(df_test))//1024
print ("Dataframe terhapus sebesar ", apus ," KB")
del df_test
del df_train
del x
print ("Proses penghapusan selesai")

print("Menghapus dataframe yang tidak terpakai . . .")
apus  = (sys.getsizeof(df2)+sys.getsizeof(df2_train)+sys.getsizeof(df2_test))//1024
print ("Dataframe terhapus sebesar ", apus ," KB")
del df2_test
del df2_train
del x2
print ("Proses penghapusan selesai")

print("Menghapus dataframe yang tidak terpakai . . .")
apus  = (sys.getsizeof(df3)+sys.getsizeof(df3_train)+sys.getsizeof(df3_test))//1024
print ("Dataframe terhapus sebesar ", apus ," KB")
del df3_test
del df3_train
del x3
print ("Proses penghapusan selesai")

"""# Proses Data Generator untuk Model LSTM
Proses ini terdiri dari beberapa step, yaitu :
1. Memangkas dataset
2. Membuat time series

## Memangkas Dataset
Pemangkasan ini bertujuan untuk membuang data yang tidak diperlukan
"""

def print_time(text, stime):
    seconds = (time.time()-stime)
    print(text, seconds//60,"menit : ",np.round(seconds%60),"detik")

def trim_dataset(mat,batch_size):
    no_of_rows_drop = mat.shape[0]%batch_size
    if no_of_rows_drop > 0:
        return mat[:-no_of_rows_drop]
    else:
        return mat

"""## Membuat Time Series
Hal ini dilakukan untuk mengubah dimensi dari dataset, yang semula 2 dimensi, menjadi 3 dimensi. Dimensi tersebut adalah input data, jumlah data yang akan dijadikan ukuran untuk back-forward dan feature yang digunakan
"""

def build_timeseries(mat, y_col_index):
    dim_0 = mat.shape[0] - TIME_STEPS
    dim_1 = mat.shape[1]
    x = np.zeros((dim_0, TIME_STEPS, dim_1))
    y = np.zeros((dim_0,))
    print("dim_0",dim_0)
    for i in tqdm_notebook(range(dim_0)):
        x[i] = mat[i:TIME_STEPS+i]
        y[i] = mat[TIME_STEPS+i, y_col_index]
    print("Ukuran time-series untuk Input dan Output : ",x.shape,y.shape)
    return x, y

# Pengecekan data yang masih null
print("Apa masih ada data yang berisikan null?",np.isnan(x_train).any(), np.isnan(x_train).any())
x_t, y_t = build_timeseries(x_train, 3)
x_t = trim_dataset(x_t, BATCH_SIZE)
y_t = trim_dataset(y_t, BATCH_SIZE)
print("Data yang sudah dipangkas menjadi : ",x_t.shape, y_t.shape)

# Pengecekan data yang masih null
print("Apa masih ada data yang berisikan null?",np.isnan(x2_train).any(), np.isnan(x2_train).any())
x2_t, y2_t = build_timeseries(x2_train, 3)
x2_t = trim_dataset(x2_t, BATCH_SIZE)
y2_t = trim_dataset(y2_t, BATCH_SIZE)
print("Data yang sudah dipangkas menjadi : ",x2_t.shape, y2_t.shape)

# Pengecekan data yang masih null
print("Apa masih ada data yang berisikan null?",np.isnan(x3_train).any(), np.isnan(x3_train).any())
x3_t, y3_t = build_timeseries(x3_train, 3)
x3_t = trim_dataset(x3_t, BATCH_SIZE)
y3_t = trim_dataset(y3_t, BATCH_SIZE)
print("Data yang sudah dipangkas menjadi : ",x3_t.shape, y3_t.shape)

# Membuat Time-Series dari data uji
x_temp, y_temp = build_timeseries(x_test, 3)
x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)
y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)

# Membuat Time-Series dari data uji
x2_temp, y2_temp = build_timeseries(x2_test, 3)
x2_val, x2_test_t = np.split(trim_dataset(x2_temp, BATCH_SIZE),2)
y2_val, y2_test_t = np.split(trim_dataset(y2_temp, BATCH_SIZE),2)

# Membuat Time-Series dari data uji
x3_temp, y3_temp = build_timeseries(x3_test, 3)
x3_val, x3_test_t = np.split(trim_dataset(x3_temp, BATCH_SIZE),2)
y3_val, y3_test_t = np.split(trim_dataset(y3_temp, BATCH_SIZE),2)

print("Banyaknya ukuran data validasi :", x_val.shape, y_val.shape)

print("Banyaknya ukuran data validasi :", x2_val.shape, y2_val.shape)

print("Banyaknya ukuran data validasi :", x3_val.shape, y3_val.shape)

"""# Membuat model LSTM
Pembuatan model ini tidak terlepas dari hyperparameter yang digunakan, banyanya sel LSTM yang dibuat dan banyaknya hidden layer yang dibuat
"""

def create_model():
    lstm_model = Sequential()
    # (batch_size, timesteps, data_dim)
    lstm_model.add(LSTM(256, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]),
                        dropout=0.0, recurrent_dropout=0.0, stateful=True, return_sequences=True,
                        kernel_initializer='random_uniform'))
    lstm_model.add(Dropout(0.4))
    lstm_model.add(LSTM(256, dropout=0.4))
    lstm_model.add(Dropout(0.4))
    lstm_model.add(Dense(128,activation='tanh'))
    lstm_model.add(Dense(1,activation='sigmoid'))
    #optimizer = optimizers.RMSprop(lr=params["lr"])
    optimizer = optimizers.Adam(lr=params["lr"], beta_1=0.9, beta_2=0.999, amsgrad=False, epsilon=None) # ini yg paling optimal
    # optimizer = optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.9, nesterov=True)
    lstm_model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])
    return lstm_model

model = None
try:
    #model = pickle.load(open("/content/drive/My Drive/TA_LSTM/outputs/lstm_model", 'rb'))
    model = pickle.load(open("lstm_model", 'rb'))
    print("Model ditemukan. Menggunakan model yang sudah ada . . .")
except FileNotFoundError:
    print("Model tidak ditemukan")

'''
if(not os.path.exists('/content/drive/My Drive/TA_LSTM/outputs/best_model.h5')):
  print("Model belum dibuat sebelumnya")
else:
  print("Model sudah dibuat sebelumnya. Model akan dimuat sesaat lagi . . .")
  model = load_model('/content/drive/My Drive/TA_LSTM/outputs/best_model.h5')
  print("Model dimuat")
'''

"""# Melakukan iterasi untuk melatih data dalam beberapa Epoch"""

is_update_model = True
if model is None or is_update_model:
    from keras import backend as K
    print("Membuat Model ...")
    print("Mengecek ketersediaan GPU ...", K.tensorflow_backend._get_available_gpus())
    model = create_model()
    
    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,
                       patience=40, min_delta=0.0001)
    
    mcp = ModelCheckpoint(os.path.join(OUTPUT_PATH,
                          "best_model.h5"), monitor='val_loss', verbose=1,
                          save_best_only=True, save_weights_only=False, mode='min', period=1)

    # Not used here. But leaving it here as a reminder for future
    r_lr_plat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=30, 
                                  verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)
    
    csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'training_log_' + time.ctime().replace(" ","_") + '.log'), append=True)
    
    history = model.fit(x_t, y_t, epochs=params["epochs"], verbose=2, batch_size=BATCH_SIZE,
                        shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),
                        trim_dataset(y_val, BATCH_SIZE)), callbacks=[es, mcp, csv_logger])
    
    print("Menyimpan model . . .")
    pickle.dump(model, open("lstm_model", "wb"))
    model.save("best_model.h5")

"""# Melakukan Prediksi
Setelah data berhasil dilatih dengan model yang sudah dibuat, maka selanjutnya adalah proses prediksi

## Prediksi dengan data yang masih Normal
"""

# Melakukan Prediksi dengan model yang sudah dibuat
y_pred = model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)
y_pred = y_pred.flatten()
y_test_t = trim_dataset(y_test_t, BATCH_SIZE)
error = mean_squared_error(y_test_t, y_pred)

# Menampilkan nilai Error menggunakan MSE
print("Besarnya nilai Error : ", error, y_pred.shape, y_test_t.shape)

"""## Proses Denormalisasi data hasil prediksi"""

# Melakukan De-normalisasi data output untuk data uji dan data validasi
y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]
y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]

"""# Visualisasi data output

## Visualisasi nilai Loss yang dihasilkan
"""

# Visualisasi model Loss dari data uji dan data validasi
from matplotlib import pyplot as plt
plt.figure(figsize=[12,6])
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.savefig(os.path.join(OUTPUT_PATH, 'train_vis_BS_'+str(BATCH_SIZE)+"_"+time.ctime()+'.png'))

# Menyimpan model
#saved_model = load_model(os.path.join(OUTPUT_PATH, '/content/drive/My Drive/TA_LSTM/outputs/best_model.h5')) 
saved_model = load_model(os.path.join(OUTPUT_PATH, 'best_model.h5')) 
print(saved_model)

y_pred = saved_model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)
y_pred = y_pred.flatten()
y_test_t = trim_dataset(y_test_t, BATCH_SIZE)
error = mean_squared_error(y_test_t, y_pred)
print("Nilai Error dari MSE sebesar ", error, y_pred.shape, y_test_t.shape)
print("Harga prediksi esok hari sebelum denormalisasi :",y_pred[0])
y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] 
print("Harga prediksi esok hari sesudah denormalisasi :",y_pred_org[0])

"""## Visualisasi hasil prediksi"""

kurang = y_pred_org - y_test_t_org
kurang_table = pd.DataFrame(kurang, columns=['hasil pengurangan uji dgn validasi'])
kurang_table.to_csv (r'/content/drive/My Drive/TA_LSTM/outputs/Hasil pengurangan ep200 8020.csv', index = None, header=True)

# Visualisasi hasil prediksi dengan model yang sudah dibuat
from matplotlib import pyplot as plt
plt.figure(figsize=[12,6])
plt.plot(y_pred_org)
plt.plot(y_test_t_org)
plt.title('Nilai Prediksi Pergerakan Harga Saham pada PT Adaro Energy Tbk')
plt.ylabel('Price (IDR)')
plt.xlabel('Validation Days')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.legend(['Prediction', 'Real'], loc='lower left')
plt.savefig(os.path.join(OUTPUT_PATH, 'pred_vs_real_BS'+str(BATCH_SIZE)+"_"+time.ctime()+'.png'))
print_time("Program selesai pada ", stime)

"""# Evaluasi Model yang sudah dibuat"""

# Evaluasi model
score = model.evaluate(x_t, y_t, batch_size=BATCH_SIZE)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""# Menyimpan hasil prediksi ke dalam csv"""

# Menyimpan hasil data prediksi ke dalam bentuk Dataframe
pred_table = pd.DataFrame(y_pred_org, columns=['Close'])

# Melakukan Ekspor data menjadi bentuk CSV
export_csv = pred_table.to_csv (r'/content/drive/My Drive/TA_LSTM/outputs/Prediksi Close Price.csv', index = None, header=True) 
print ("Dokumen CSV sudah tersimpan di direktori", OUTPUT_PATH)

# Menggabungkan dataset dengan data lama
df_close = df.drop(columns=['Date','Open', 'High', 'Low', 'Adj Close', 'Volume'])
df_new = pd.concat([df_close, pred_table], ignore_index=True, sort=True)

# Menyimpan dataset baru
export_csv = df_new.to_csv(r'/content/drive/My Drive/TA_LSTM/outputs/Dataset baru.csv', index = None, header=True) 
print ("Dokumen CSV sudah tersimpan di direktori", OUTPUT_PATH)